{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb398ce-b4c2-45cc-94e0-bb3fa4e9691a",
   "metadata": {},
   "source": [
    "## Support Vector Machines-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2e75a7-48a5-450e-ad6e-b63a30946f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce54ba6-7788-4907-bdad-f720e79fe1a1",
   "metadata": {},
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) can be represented as follows:\n",
    "\n",
    "Given a set of training data \n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "(x \n",
    "i\n",
    "​\n",
    " ,y \n",
    "i\n",
    "​\n",
    " ) where \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  represents the input features and \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  represents the corresponding class label (+1 or -1 for binary classification), a linear SVM aims to find the optimal hyperplane that separates the data into different classes. The equation for the hyperplane can be expressed as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "w\n",
    "⋅\n",
    "x\n",
    "+\n",
    "�\n",
    "f(x)=w⋅x+b\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "f(x) represents the decision function that assigns input \n",
    "�\n",
    "x to a particular class based on the sign of the function.\n",
    "w\n",
    "w is the weight vector (perpendicular to the hyperplane) that determines the orientation of the hyperplane.\n",
    "x\n",
    "x is the input vector.\n",
    "�\n",
    "b is the bias term (or intercept) which shifts the hyperplane away from the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa978135-4f53-4cdc-a7f8-b01d19372c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c948f1f4-3d3c-4dd5-8167-7b21a3d4bc0f",
   "metadata": {},
   "source": [
    "The primary goal of a linear SVM is to maximize the margin, which is the distance between the hyperplane and the nearest data points (support vectors) from each class. Mathematically, the objective function of a linear SVM is to minimize the norm of the weight vector (\n",
    "w\n",
    "w) while correctly classifying the training data with a margin of at least 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c1b1f-209b-430a-814a-ea2befda1747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167c4a87-a18d-4fb9-b031-656a19a18d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec48afb-e6fa-4f89-b563-9b96aee17926",
   "metadata": {},
   "source": [
    "The kernel trick is a concept used in Support Vector Machines (SVMs) to handle non-linearly separable data by implicitly transforming the input features into a higher-dimensional space without explicitly calculating the transformed feature vectors. It allows SVMs to perform non-linear classification by using a kernel function, which computes the dot product between the transformed feature vectors in the higher-dimensional space efficiently.\n",
    "\n",
    "In essence, instead of explicitly mapping the input features to a higher-dimensional space (which might be computationally expensive or even infeasible for very high dimensions), the kernel trick enables the SVM to work directly in the original feature space but implicitly computes the dot product between the transformed vectors in the higher-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89736fff-b9cc-4dcb-bba0-6e34138264bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a84278-bbdd-4b8a-8f35-d47ebf337bdb",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVMs), support vectors play a crucial role in defining the decision boundary (hyperplane) and determining the optimal separation between different classes of data points.\n",
    "\n",
    "Support vectors are the data points from the training set that lie closest to the decision boundary, also known as the hyperplane. They are the critical elements that directly influence the positioning and orientation of the hyperplane. These vectors support the definition of the decision boundary because if any of these support vectors were removed or altered, it could potentially change the position or orientation of the hyperplane, impacting the classification of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e832d352-03a1-4993-84d6-01d877b1df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104dba68-8118-42a1-ab86-698e3593975b",
   "metadata": {},
   "source": [
    "Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVMs, I'll use a simple 2D dataset and demonstrate these concepts with graphical representations.\n",
    "\n",
    "Let's consider a toy dataset with two classes (Class A and Class B) that are not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b3ddb-fded-4d61-9919-d541232dbac0",
   "metadata": {},
   "source": [
    "Hyperplane:\n",
    "The hyperplane is the decision boundary that separates the two classes in an SVM. In a 2D space, it's a line. In this example, since the classes are not linearly separable, we can't draw a single straight line that perfectly separates them. The hyperplane's goal is to maximize the margin between the classes.\n",
    "\n",
    "Marginal Plane:\n",
    "The marginal planes are parallel planes to the hyperplane that run parallel to it and just touch the support vectors from both classes. They define the margins in an SVM.\n",
    "\n",
    "Soft Margin and Hard Margin:\n",
    "Hard Margin: This refers to a strict margin where the SVM doesn't allow any misclassifications. If the data is not linearly separable, a hard-margin SVM won't be able to find a solution.\n",
    "Soft Margin: A soft-margin SVM allows for some misclassifications to find a better decision boundary. It uses a penalty parameter (C) that controls the trade-off between maximizing the margin and allowing misclassifications."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ad03902-c099-4fb0-962c-526b998ed363",
   "metadata": {},
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980022b-6377-4cd0-868f-80dac8568605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
